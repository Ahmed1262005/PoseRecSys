"""
BERT4Rec Ranker for Stage 2 of the two-stage recommendation system.

Uses a trained BERT4Rec model to score and rank candidates
generated by the CandidateGenerator (Stage 1).
"""
import os
import sys
import torch
import numpy as np
from typing import List, Tuple, Dict, Optional
from pathlib import Path

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


class BERT4RecRanker:
    """
    Stage 2 Ranker using trained BERT4Rec model.

    BERT4Rec uses bidirectional self-attention on user's item sequence
    to predict the next item. This class wraps the trained model for
    efficient inference on candidate items.

    Performance target: ~50ms to rank 1000 candidates
    """

    def __init__(
        self,
        checkpoint_path: str,
        device: Optional[str] = None
    ):
        """
        Initialize ranker with trained BERT4Rec checkpoint.

        Args:
            checkpoint_path: Path to RecBole BERT4Rec checkpoint (.pth)
            device: Device to use ('cuda', 'cpu', or None for auto)
        """
        print(f"Loading BERT4Rec model from: {checkpoint_path}")

        # Load model and associated data using custom loader for PyTorch 2.6+
        self.config, self.model, self.dataset, self.train_data, self.valid_data, self.test_data = \
            self._load_model_compat(checkpoint_path)

        # Set device
        if device is None:
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.device = device
        self.model = self.model.to(self.device)
        self.model.eval()

        # Build mappings
        self._build_mappings()

        print(f"  Model loaded on device: {self.device}")
        print(f"  Number of items: {self.n_items}")
        print(f"  Number of users: {self.n_users}")
        print(f"  Max sequence length: {self.max_seq_len}")

    def _load_model_compat(self, checkpoint_path: str):
        """
        Load RecBole model with PyTorch 2.6+ compatibility.

        PyTorch 2.6 changed the default for weights_only in torch.load.
        This method patches the loader to work with RecBole checkpoints.
        """
        import recbole.quick_start.quick_start as qs
        from recbole.config import Config
        from recbole.data import create_dataset, data_preparation
        from recbole.utils import get_model, init_seed

        # Load checkpoint with weights_only=False for RecBole compatibility
        checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)

        config = checkpoint['config']
        init_seed(config['seed'], config['reproducibility'])

        # Create dataset and dataloaders
        dataset = create_dataset(config)
        train_data, valid_data, test_data = data_preparation(config, dataset)

        # Load model
        model = get_model(config['model'])(config, train_data.dataset).to(config['device'])
        model.load_state_dict(checkpoint['state_dict'])
        model.load_other_parameter(checkpoint.get('other_parameter', None))

        return config, model, dataset, train_data, valid_data, test_data

    def _build_mappings(self):
        """Build item ID to internal index mappings."""
        # Item token to internal ID
        self.item_token2id = self.dataset.field2token_id['item_id']
        self.item_id2token = self.dataset.field2id_token['item_id']

        # User token to internal ID
        self.user_token2id = self.dataset.field2token_id['user_id']
        self.user_id2token = self.dataset.field2id_token['user_id']

        # Model dimensions
        self.n_items = self.dataset.item_num
        self.n_users = self.dataset.user_num
        self.max_seq_len = self.config['MAX_ITEM_LIST_LENGTH']

    def _get_user_history(self, user_id: str) -> List[int]:
        """
        Get user's interaction history from the dataset.

        Args:
            user_id: User ID string

        Returns:
            List of internal item IDs the user has interacted with
        """
        user_token = self.user_token2id.get(user_id)
        if user_token is None:
            return []

        # Get history from train data
        history = []
        if self.train_data is not None:
            uid_field = self.train_data.dataset.uid_field
            iid_field = self.train_data.dataset.iid_field

            inter_feat = self.train_data.dataset.inter_feat
            mask = inter_feat[uid_field] == user_token

            if mask.sum() > 0:
                history = inter_feat[iid_field][mask].tolist()

        return history

    def _build_sequence_tensor(
        self,
        item_sequence: List[int],
        max_len: Optional[int] = None
    ) -> torch.Tensor:
        """
        Build padded sequence tensor for BERT4Rec input.

        Args:
            item_sequence: List of internal item IDs
            max_len: Maximum sequence length (uses model default if None)

        Returns:
            Tensor of shape (1, max_len) with padding
        """
        if max_len is None:
            max_len = self.max_seq_len

        # Truncate if too long
        if len(item_sequence) > max_len:
            item_sequence = item_sequence[-max_len:]

        # Pad if too short
        padded = [0] * (max_len - len(item_sequence)) + item_sequence

        return torch.tensor([padded], dtype=torch.long, device=self.device)

    def rank_candidates(
        self,
        user_id: str,
        candidates: List[int],
        user_history: Optional[List[int]] = None,
        k: int = 30
    ) -> List[Tuple[int, float]]:
        """
        Stage 2: Score and rank candidates using BERT4Rec.

        Args:
            user_id: User identifier string (e.g., 'u123')
            candidates: List of candidate item IDs from Stage 1 (original IDs)
            user_history: Optional user history (if None, loaded from dataset)
            k: Number of top items to return

        Returns:
            List of (item_id, score) tuples sorted by score descending
        """
        # Get or load user history
        if user_history is None:
            # Convert user_id to internal representation
            internal_history = self._get_user_history(user_id)
        else:
            # Convert external item IDs to internal
            internal_history = []
            for item_id in user_history:
                token = str(item_id)
                if token in self.item_token2id:
                    internal_history.append(self.item_token2id[token])

        if not internal_history:
            # Cold start user: return candidates with uniform scores
            return [(c, 0.0) for c in candidates[:k]]

        # Build input sequence
        seq_tensor = self._build_sequence_tensor(internal_history)
        seq_len = min(len(internal_history), self.max_seq_len)

        # Get scores from model
        with torch.no_grad():
            # BERT4Rec expects specific field names based on ITEM_ID_FIELD config
            # Default naming: item_id_list and item_length
            item_seq_field = self.config['ITEM_ID_FIELD'] + '_list'
            item_len_field = 'item_length'

            interaction = {
                item_seq_field: seq_tensor,
                item_len_field: torch.tensor([seq_len], device=self.device),
            }

            # Get all item scores
            scores = self.model.full_sort_predict(interaction)  # Shape: (1, n_items)
            scores = scores.squeeze(0).cpu().numpy()

        # Map candidates to internal indices and get scores
        candidate_scores = []
        for item_id in candidates:
            token = str(item_id)
            if token in self.item_token2id:
                internal_idx = self.item_token2id[token]
                score = float(scores[internal_idx])
                candidate_scores.append((item_id, score))
            else:
                # Unknown item: assign minimum score
                candidate_scores.append((item_id, float('-inf')))

        # Sort by score descending and return top-k
        ranked = sorted(candidate_scores, key=lambda x: x[1], reverse=True)

        return ranked[:k]

    def batch_rank_candidates(
        self,
        user_ids: List[str],
        candidates_list: List[List[int]],
        user_histories: Optional[List[List[int]]] = None,
        k: int = 30
    ) -> List[List[Tuple[int, float]]]:
        """
        Batch ranking for multiple users.

        Args:
            user_ids: List of user ID strings
            candidates_list: List of candidate lists (one per user)
            user_histories: Optional list of user histories
            k: Number of top items per user

        Returns:
            List of ranked results (one list of (item_id, score) per user)
        """
        results = []

        for i, user_id in enumerate(user_ids):
            candidates = candidates_list[i]
            history = user_histories[i] if user_histories else None

            ranked = self.rank_candidates(
                user_id=user_id,
                candidates=candidates,
                user_history=history,
                k=k
            )
            results.append(ranked)

        return results

    def get_item_scores(
        self,
        user_history: List[int]
    ) -> np.ndarray:
        """
        Get scores for all items given a user history.

        Args:
            user_history: List of item IDs (original, not internal)

        Returns:
            numpy array of scores for all items (internal indexing)
        """
        # Convert to internal IDs
        internal_history = []
        for item_id in user_history:
            token = str(item_id)
            if token in self.item_token2id:
                internal_history.append(self.item_token2id[token])

        if not internal_history:
            return np.zeros(self.n_items)

        # Build sequence tensor
        seq_tensor = self._build_sequence_tensor(internal_history)
        seq_len = torch.tensor([len(internal_history)], device=self.device)

        with torch.no_grad():
            interaction = {
                'item_seq': seq_tensor,
                'item_seq_len': seq_len,
            }
            scores = self.model.full_sort_predict(interaction)
            scores = scores.squeeze(0).cpu().numpy()

        return scores


def find_latest_bert4rec_checkpoint(model_dir: str = "models") -> Optional[str]:
    """Find the most recent BERT4Rec checkpoint."""
    model_path = Path(model_dir)
    if not model_path.exists():
        return None

    checkpoints = list(model_path.glob("BERT4Rec*.pth"))
    if not checkpoints:
        return None

    # Return most recent by modification time
    return str(max(checkpoints, key=lambda p: p.stat().st_mtime))


def main():
    """Test BERT4Rec ranker."""
    print("=" * 60)
    print("Testing BERT4Rec Ranker")
    print("=" * 60)

    # Find checkpoint
    checkpoint = find_latest_bert4rec_checkpoint()
    if not checkpoint:
        print("No BERT4Rec checkpoint found in models/")
        print("Please train the model first using: python src/train_models.py BERT4Rec")
        return

    print(f"Using checkpoint: {checkpoint}")

    # Initialize ranker
    ranker = BERT4RecRanker(checkpoint)

    # Get sample users from the dataset
    sample_users = list(ranker.user_token2id.keys())[:5]
    print(f"\nSample users from dataset: {sample_users}")

    # Test ranking
    print("\n--- Test 1: Rank candidates for a user ---")
    if sample_users:
        user_id = sample_users[1] if len(sample_users) > 1 else sample_users[0]
        print(f"User: {user_id}")

        # Get user's history
        history = ranker._get_user_history(user_id)
        print(f"User history (internal IDs): {history[:10]}..." if len(history) > 10 else f"User history: {history}")

        # Create fake candidates (mix of real items)
        sample_items = list(ranker.item_token2id.keys())[1:1001]  # Get 1000 items
        candidates = [int(item) for item in sample_items if item.isdigit()][:1000]

        import time
        start = time.time()
        ranked = ranker.rank_candidates(
            user_id=user_id,
            candidates=candidates,
            k=20
        )
        elapsed = (time.time() - start) * 1000

        print(f"\nRanked {len(candidates)} candidates in {elapsed:.2f}ms")
        print("Top 10 ranked items:")
        for i, (item_id, score) in enumerate(ranked[:10]):
            print(f"  {i+1}. Item {item_id}: {score:.4f}")

    # Test with custom history
    print("\n--- Test 2: Rank with custom history ---")
    custom_history = [100, 200, 300, 400, 500]
    candidates = list(range(1000, 2000))

    import time
    start = time.time()
    ranked = ranker.rank_candidates(
        user_id="custom_user",
        candidates=candidates,
        user_history=custom_history,
        k=10
    )
    elapsed = (time.time() - start) * 1000

    print(f"Custom history: {custom_history}")
    print(f"Ranked {len(candidates)} candidates in {elapsed:.2f}ms")
    print("Top 10:")
    for i, (item_id, score) in enumerate(ranked):
        print(f"  {i+1}. Item {item_id}: {score:.4f}")

    # Performance test
    print("\n--- Test 3: Performance benchmark ---")
    n_queries = 100
    candidates = list(range(100, 1100))  # 1000 candidates

    start = time.time()
    for _ in range(n_queries):
        ranker.rank_candidates(
            user_id="test_user",
            candidates=candidates,
            user_history=custom_history,
            k=30
        )
    elapsed = (time.time() - start) * 1000

    print(f"{n_queries} ranking queries completed in {elapsed:.2f}ms")
    print(f"Average latency: {elapsed/n_queries:.3f}ms per query")

    print("\n" + "=" * 60)
    print("BERT4Rec Ranker tests passed!")
    print("=" * 60)


if __name__ == '__main__':
    main()
